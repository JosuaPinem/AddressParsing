{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "308acb06",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ‡®ðŸ‡© Address NER with IndoBERT â€” Endâ€‘toâ€‘End Notebook\n",
    "\n",
    "Notebook ini membangun model **NER (BIO)** untuk parsing alamat Indonesia menggunakan **Hugging Face** dan **`indobenchmark/indobert-base-p1`**.\n",
    "\n",
    "**Rangka:**\n",
    "1. **Data Preprocessing** â€” load, validasi, EDA (statistik deskriptif).\n",
    "2. **Splitting Data** â€” pembagian train/valid/test yang stabil dan representatif.\n",
    "3. **Data Processing & Training** â€” tokenisasi, label alignment, training dengan `Trainer`, simpan model.\n",
    "4. **Evaluation & Visualization** â€” metrik `seqeval`, grafik training loss & F1, laporan ringkas.\n",
    "\n",
    "> Catatan: pastikan file dataset JSON kamu (hasil generator) tersedia. Notebook akan mencari otomatis beberapa nama file umum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc4a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%capture\n",
    "# Jika menjalankan di lingkungan baru (Colab/venv), uncomment baris berikut:\n",
    "# !pip install -q transformers datasets seqeval accelerate matplotlib\n",
    "\n",
    "import os, json, random, math, collections\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification,\n",
    "                          TrainingArguments, Trainer, EarlyStoppingCallback)\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "MODEL_NAME = \"indobenchmark/indobert-base-p1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9393d",
   "metadata": {},
   "source": [
    "## 1) Data Preprocessing â€” Load, Validasi, dan EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26483ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cari file dataset secara otomatis\n",
    "CANDIDATE_FILES = [\n",
    "    \"alamat_dataset_1200.json\",\n",
    "    \"alamat_dataset_1000.json\",\n",
    "    \"alamat_dataset.json\",\n",
    "    \"base.json\"\n",
    "]\n",
    "\n",
    "data_path = None\n",
    "for f in CANDIDATE_FILES:\n",
    "    if os.path.exists(f):\n",
    "        data_path = f\n",
    "        break\n",
    "\n",
    "if data_path is None:\n",
    "    # Fallback: buat contoh mini jika file tidak ditemukan\n",
    "    data_path = \"mini_sample.json\"\n",
    "    mini = [\n",
    "        {\"id\": 1, \"token\": [\"Jl.\", \"Diponegoro\", \"No.15\", \",\", \"Kel.\", \"Menteng\", \",\", \"Kec.\", \"Menteng\", \",\", \"Kota\", \"Jakarta\", \"Pusat\", \",\", \"DKI\", \"Jakarta\"], \n",
    "         \"labels\": [\"B-JALAN\",\"I-JALAN\",\"I-JALAN\",\"O\",\"B-KELURAHAN\",\"I-KELURAHAN\",\"O\",\"B-KECAMATAN\",\"I-KECAMATAN\",\"O\",\"B-KOTA\",\"I-KOTA\",\"I-KOTA\",\"O\",\"B-PROVINSI\",\"I-PROVINSI\"]},\n",
    "        {\"id\": 2, \"token\": [\"Gg.\", \"Mawar\", \"No.7\", \"RT.\", \"01\", \"/\", \"RW.\", \"02\", \",\", \"Kel.\", \"Sukajadi\", \",\", \"Kec.\", \"Sukajadi\", \",\", \"Kota\", \"Pekanbaru\", \",\", \"Riau\"], \n",
    "         \"labels\": [\"B-JALAN\",\"I-JALAN\",\"I-JALAN\",\"B-RT\",\"I-RT\",\"O\",\"B-RW\",\"I-RW\",\"O\",\"B-KELURAHAN\",\"I-KELURAHAN\",\"O\",\"B-KECAMATAN\",\"I-KECAMATAN\",\"O\",\"B-KOTA\",\"I-KOTA\",\"B-PROVINSI\"]}\n",
    "    ]\n",
    "    with open(data_path, \"w\", encoding=\"utf8\") as f:\n",
    "        json.dump(mini, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded file: {data_path} â€” {len(raw_data)} rows\")\n",
    "\n",
    "# Validasi struktur minimal\n",
    "assert all(isinstance(r, dict) and \"token\" in r and \"labels\" in r for r in raw_data)\n",
    "\n",
    "# EDA ringkas\n",
    "num_examples = len(raw_data)\n",
    "token_lens = [len(r[\"token\"]) for r in raw_data]\n",
    "label_counts = collections.Counter([lab for r in raw_data for lab in r[\"labels\"]])\n",
    "entity_spans = collections.Counter([lab.split('-')[-1] for lab in label_counts if lab != \"O\"])\n",
    "\n",
    "print(\"\\nStatistik dasar:\")\n",
    "print(f\"- Jumlah contoh: {num_examples}\")\n",
    "print(f\"- Rata-rata panjang token: {np.mean(token_lens):.2f} Â± {np.std(token_lens):.2f}\")\n",
    "print(f\"- Min/Max panjang token: {np.min(token_lens)} / {np.max(token_lens)}\")\n",
    "\n",
    "print(\"\\n10 label teratas:\")\n",
    "for k, v in label_counts.most_common(10):\n",
    "    print(f\"{k:<12} : {v}\")\n",
    "\n",
    "# Visualisasi panjang sequence\n",
    "plt.figure()\n",
    "plt.hist(token_lens, bins=30)\n",
    "plt.title(\"Distribusi Panjang Token per Alamat\")\n",
    "plt.xlabel(\"Jumlah token\")\n",
    "plt.ylabel(\"Frekuensi\")\n",
    "plt.show()\n",
    "\n",
    "# Info khusus: proporsi yang memiliki RT/RW, Kel/KeC, Kota, Prov\n",
    "def has_any(labels: List[str], prefix: str) -> bool:\n",
    "    return any(l.startswith(prefix) for l in labels)\n",
    "\n",
    "has_rt = sum(has_any(r[\"labels\"], \"B-RT\") for r in raw_data)\n",
    "has_rw = sum(has_any(r[\"labels\"], \"B-RW\") for r in raw_data)\n",
    "has_kel = sum(has_any(r[\"labels\"], \"B-KELURAHAN\") for r in raw_data)\n",
    "has_kec = sum(has_any(r[\"labels\"], \"B-KECAMATAN\") for r in raw_data)\n",
    "has_kota = sum(has_any(r[\"labels\"], \"B-KOTA\") for r in raw_data)\n",
    "has_prov = sum(has_any(r[\"labels\"], \"B-PROVINSI\") for r in raw_data)\n",
    "has_kodepos = sum(has_any(r[\"labels\"], \"B-KODEPOS\") for r in raw_data)\n",
    "\n",
    "print(\"\\nKomponen alamat (jumlah & proporsi):\")\n",
    "print(f\"- RT: {has_rt} ({has_rt/num_examples:.1%}) | RW: {has_rw} ({has_rw/num_examples:.1%})\")\n",
    "print(f\"- Kelurahan: {has_kel} ({has_kel/num_examples:.1%}) | Kecamatan: {has_kec} ({has_kec/num_examples:.1%})\")\n",
    "print(f\"- Kota: {has_kota} ({has_kota/num_examples:.1%}) | Provinsi: {has_prov} ({has_prov/num_examples:.1%})\")\n",
    "print(f\"- Kodepos: {has_kodepos} ({has_kodepos/num_examples:.1%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302736cc",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Splitting Data â€” Train/Validation/Test\n",
    "\n",
    "Strategi: **80/10/10** dengan seed tetap agar reprodusibel. Untuk NER, stratifikasi sempurna sulit (karena sekuens), \n",
    "tapi kita jaga **distribusi panjang** dan keberadaan komponen alamat tetap mirip antar split dengan **shuffling terkontrol**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a1ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shuffle dengan seed tetap\n",
    "idx = list(range(len(raw_data)))\n",
    "random.shuffle(idx)\n",
    "\n",
    "train_end = int(0.8 * len(idx))\n",
    "val_end = int(0.9 * len(idx))\n",
    "\n",
    "train_idx = idx[:train_end]\n",
    "val_idx = idx[train_end:val_end]\n",
    "test_idx = idx[val_end:]\n",
    "\n",
    "def select(data, indices):\n",
    "    return [data[i] for i in indices]\n",
    "\n",
    "train_data = select(raw_data, train_idx)\n",
    "val_data = select(raw_data, val_idx)\n",
    "test_data = select(raw_data, test_idx)\n",
    "\n",
    "print(f\"Split sizes -> train: {len(train_data)}, val: {len(val_data)}, test: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa23c5eb",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Data Processing & Training (Hugging Face)\n",
    "\n",
    "- Tokenizer/model: **`indobenchmark/indobert-base-p1`**\n",
    "- Batch size: 16 (ubah jika GPU/VRAM terbatas)\n",
    "- LR: 2e-5\n",
    "- **Epochs rekomendasi**: `6` dengan **early stopping (patience=2)** â†’ mencegah overfitting pada 1200 data.\n",
    "- Simpan model terbaik berdasarkan `eval_f1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbdcf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Siapkan label set dan mapping\n",
    "all_labels = sorted(list(set(l for r in raw_data for l in r[\"labels\"])))\n",
    "# Pastikan konsistensi skema BIO\n",
    "if \"O\" in all_labels:\n",
    "    # biarkan O paling awal hanya untuk kenyamanan display (opsional)\n",
    "    all_labels.remove(\"O\")\n",
    "    all_labels = [\"O\"] + all_labels\n",
    "\n",
    "label2id = {l:i for i,l in enumerate(all_labels)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "print(\"Labels:\", all_labels)\n",
    "print(\"num_labels:\", len(all_labels))\n",
    "\n",
    "# Helper: convert list-of-dicts to HF Dataset\n",
    "def to_hf_dataset(rows):\n",
    "    return Dataset.from_dict({\n",
    "        \"id\": [r.get(\"id\", -1) for r in rows],\n",
    "        \"tokens\": [r[\"token\"] for r in rows],\n",
    "        \"ner_tags\": [[label2id[l] for l in r[\"labels\"]] for r in rows],\n",
    "    })\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": to_hf_dataset(train_data),\n",
    "    \"validation\": to_hf_dataset(val_data),\n",
    "    \"test\": to_hf_dataset(test_data),\n",
    "})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Align labels dengan wordpiece tokenizer\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    aligned_labels = []\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # diabaikan loss\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(labels[word_idx])\n",
    "            else:\n",
    "                # untuk subword lanjutan â†’ gunakan skema I-*\n",
    "                lab_id = labels[word_idx]\n",
    "                lab = id2label[lab_id]\n",
    "                if lab == \"O\":\n",
    "                    label_ids.append(lab_id)\n",
    "                else:\n",
    "                    # paksa ke I- untuk subword lanjutan\n",
    "                    ent = lab.split(\"-\", 1)[-1]\n",
    "                    label_ids.append(label2id.get(\"I-\" + ent, lab_id))\n",
    "            previous_word_idx = word_idx\n",
    "        aligned_labels.append(label_ids)\n",
    "    tokenized[\"labels\"] = aligned_labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(all_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "metric_best = \"f1\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"indoaddr_ner_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_best,\n",
    "    report_to=\"none\",\n",
    "    seed=RANDOM_SEED,\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "# Metrik seqeval\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    for pred, lab in zip(preds, labels):\n",
    "        curr_pred = []\n",
    "        curr_lab = []\n",
    "        for p_i, l_i in zip(pred, lab):\n",
    "            if l_i == -100:\n",
    "                continue\n",
    "            curr_pred.append(id2label[p_i])\n",
    "            curr_lab.append(id2label[l_i])\n",
    "        true_predictions.append(curr_pred)\n",
    "        true_labels.append(curr_lab)\n",
    "\n",
    "    f1 = f1_score(true_labels, true_predictions, scheme=IOB2)\n",
    "    prec = precision_score(true_labels, true_predictions, scheme=IOB2)\n",
    "    rec = recall_score(true_labels, true_predictions, scheme=IOB2)\n",
    "    return {\"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "trainer.save_model(\"indoaddr_ner_model/best\")  # simpan model terbaik\n",
    "tokenizer.save_pretrained(\"indoaddr_ner_model/best\")\n",
    "print(\"Model saved to: indoaddr_ner_model/best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd599551",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Evaluation & Visualization\n",
    "\n",
    "Kita evaluasi di **test set** dan tampilkan:\n",
    "- **Classification report** (precision / recall / F1 per label)\n",
    "- **Grafik training & eval loss per epoch**\n",
    "- **Grafik F1 validasi per epoch**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluasi test set\n",
    "test_metrics = trainer.evaluate(tokenized_ds[\"test\"])\n",
    "print(\"Test metrics:\", test_metrics)\n",
    "\n",
    "# Classification report (per label)\n",
    "preds_raw, labels_raw, _ = trainer.predict(tokenized_ds[\"test\"])\n",
    "pred_ids = np.argmax(preds_raw, axis=-1)\n",
    "\n",
    "true_predictions = []\n",
    "true_labels = []\n",
    "for pred, lab in zip(pred_ids, labels_raw):\n",
    "    curr_pred, curr_lab = [], []\n",
    "    for p_i, l_i in zip(pred, lab):\n",
    "        if l_i == -100:\n",
    "            continue\n",
    "        curr_pred.append(id2label[p_i])\n",
    "        curr_lab.append(id2label[l_i])\n",
    "    true_predictions.append(curr_pred)\n",
    "    true_labels.append(curr_lab)\n",
    "\n",
    "print(\"\\nClassification Report (seqeval):\")\n",
    "print(classification_report(true_labels, true_predictions, scheme=IOB2))\n",
    "\n",
    "# Ambil history training dari trainer.state.log_history\n",
    "logs = trainer.state.log_history\n",
    "# Ekstrak loss/eval stats per epoch\n",
    "epochs = []\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "eval_f1 = []\n",
    "\n",
    "for rec in logs:\n",
    "    if \"epoch\" in rec:\n",
    "        ep = rec[\"epoch\"]\n",
    "        if \"loss\" in rec:\n",
    "            epochs.append(ep)\n",
    "            train_loss.append(rec[\"loss\"])\n",
    "        if \"eval_loss\" in rec:\n",
    "            eval_loss.append(rec[\"eval_loss\"])\n",
    "        if \"eval_f1\" in rec:\n",
    "            eval_f1.append(rec[\"eval_f1\"])\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure()\n",
    "plt.plot(epochs[:len(train_loss)], train_loss, marker=\"o\")\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot eval loss\n",
    "plt.figure()\n",
    "plt.plot(epochs[:len(eval_loss)], eval_loss, marker=\"o\")\n",
    "plt.title(\"Validation Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot eval F1\n",
    "plt.figure()\n",
    "plt.plot(epochs[:len(eval_f1)], eval_f1, marker=\"o\")\n",
    "plt.title(\"Validation F1 per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"F1\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
